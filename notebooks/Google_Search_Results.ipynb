{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from google import google\n",
    "import gnp\n",
    "import codecs\n",
    "import json\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.key import Key\n",
    "import boto\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "from TwitterSearch import *\n",
    "import sunlight\n",
    "from sunlight import congress\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import itertools\n",
    "import datetime\n",
    "import configparser\n",
    "import time\n",
    "from pattern.en import parsetree\n",
    "from pattern.en import mood\n",
    "\n",
    "from googleplaces import GooglePlaces, types, lang\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import requests\n",
    "import urllib, simplejson\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            city state population            full_city\n",
      "0      New York     NY  8,363,710      \"New York , NY\"\n",
      "1   Los Angeles     CA  3,833,995   \"Los Angeles , CA\"\n",
      "2       Chicago     IL  2,853,114       \"Chicago , IL\"\n",
      "3       Houston     TX  2,242,193       \"Houston , TX\"\n",
      "4       Phoenix     AZ  1,567,924       \"Phoenix , AZ\"\n",
      "5  Philadelphia     PA  1,447,395  \"Philadelphia , PA\"\n",
      "6   San Antonio     TX  1,351,305   \"San Antonio , TX\"\n",
      "7        Dallas     TX  1,279,910        \"Dallas , TX\"\n",
      "8     San Diego     CA  1,279,329     \"San Diego , CA\"\n",
      "9      San Jose     CA    948,279      \"San Jose , CA\"\n",
      "['\"New York , NY\"', '\"Los Angeles , CA\"', '\"Chicago , IL\"', '\"Houston , TX\"', '\"Phoenix , AZ\"', '\"Philadelphia , PA\"', '\"San Antonio , TX\"', '\"Dallas , TX\"', '\"San Diego , CA\"', '\"San Jose , CA\"']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'Jersey City, NJ' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-9ed26a61d7e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcity_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_city'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mcity_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mcity_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Jersey City, NJ'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: 'Jersey City, NJ' is not in list"
     ]
    }
   ],
   "source": [
    "cities = pd.read_csv(os.path.join(os.pardir, 'data/static_data', 'Top5000Population.csv'))\n",
    "cities['full_city'] = '\\\"' + cities['city'] + ', ' + cities['state'] + '\\\"'\n",
    "print cities[:10]\n",
    "city_list = list(cities['full_city'])\n",
    "print city_list[:10]\n",
    "print city_list.index('Jersey City, NJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['protest', '\"Nashville, TN\"'], ['protest', '\"Washington , DC\"'], ['protest', '\"Las Vegas , NV\"'], ['protest', '\"Portland , OR\"'], ['protest', '\"Louisville, KY\"'], ['protest', '\"Oklahoma City , OK\"'], ['protest', '\"Tucson , AZ\"'], ['protest', '\"Atlanta , GA\"'], ['protest', '\"Albuquerque , NM\"'], ['protest', '\"Fresno , CA\"'], ['protest', '\"Sacramento , CA\"'], ['protest', '\"Long Beach , CA\"'], ['protest', '\"Mesa , AZ\"'], ['protest', '\"Kansas City , MO\"'], ['protest', '\"Omaha , NE\"'], ['protest', '\"Cleveland , OH\"'], ['protest', '\"Virginia Beach , VA\"'], ['protest', '\"Miami , FL\"'], ['protest', '\"Oakland , CA\"'], ['protest', '\"Raleigh , NC\"']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "action_categories = {\n",
    "                     'Townhall': [\"indivisible\",'\\\"town hall\\\"'],\n",
    "#                        'Donation': [\"donation\",\"donate\",\"give support\",\"financial support\",\"contribute\",\"contributing\",\"give a contribution\"],\n",
    "                     'Protest': [\"protest\",\"rally\",\"demonstration\"],\n",
    "#                      'Gathering': [\"event\"],\n",
    "#                                    ,\"meetup\",\"huddle\",\"congregate\",\"gather\",\"gathering\",\"discuss\",\"discussion\", \"indivisible\"],\n",
    "                     'Boycott': [\"boycott\"]\n",
    "#                      'Advocate': [\"call\",\"email\",\"reach out\", \"sign petition\",\"petition\"],\n",
    "#                      'Vote': [\"vote\",\"cast your ballot\"]\n",
    "                          }\n",
    "\n",
    "issue_types = { 'immigrant rights' : ['immigrant rights',\"immigrants' rights\",'refugee rights', 'travel ban',\n",
    "                                        'border wall','refugees','asylum','immigration reform',\n",
    "                                        'immigrant advocacy','migrant rights','undocumented'],\n",
    "               \"women's rights\" : [\"women's rights\",\"women's rights\",'womens rights','gender equality'\n",
    "                                  'girl power',\"international women's day\",\"war on women\",\"planned parenthood\"],\n",
    "               \"black rights\" : [\"racial equality\",\"black lives matter\",\"african american rights\",\"civil rights\",\n",
    "                                \"black power\",\"jim crow\"],\n",
    "               \"LGTBQ rights\" : [\"marriage equality\",\"transgender rights\",\"equality act\",\"lesbian rights\",'gay rights',\n",
    "                                'bisexual rights'],\n",
    "               \"voting rights\": [\"redistricting\",\"gerrymandering\",\"redistrict\",\"gerrymander\",\"voter id\",\"voting access\",\n",
    "                                \"voter access\",\"voter suppression\"]\n",
    "                      }\n",
    "\n",
    "\n",
    "legislators = pd.read_csv(os.path.join(os.pardir, 'data/static_data', 'legislators.csv'))\n",
    "\n",
    "mask = (legislators.in_office == 1)\n",
    "\n",
    "legislators = legislators.loc[mask]\n",
    "\n",
    "legislators['title_name'] = '\\\"' + legislators['full_title'] + ' ' + legislators['firstname'] + ' ' + legislators['lastname'] + '\\\"'\n",
    "\n",
    "# print legislators.columns\n",
    "# print\n",
    "# print list(legislators['title_name'])[:10]\n",
    "\n",
    "leg_names = list(legislators['title_name'])\n",
    "# leg_states = list(legislators['state'])\n",
    "\n",
    "keyword_searches = []\n",
    "for category in action_categories:\n",
    "    for combination in itertools.product( action_categories[category],city_list[5:]):\n",
    "      keyword_searches.append(list(combination))\n",
    "  \n",
    "print keyword_searches[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /usr/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.facebook.com/Dayton-Womens-Rights-Alliance-589846831145259/'\n",
    "header = {'User-Agent': 'Mozilla/5.0'} #Needed to prevent 403 error on Wikipedia\n",
    "req = urllib2.Request(url,headers=header)\n",
    "page = urllib2.urlopen(req)\n",
    "soup = BeautifulSoup(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 3 ways you can participate in #DayWithoutAWoman on March 8th: 1. Women take the day off, from paid and unpaid labor 2. Avoid shopping for one day (with exceptions for small, women- and minority-owned businesses) 3. Wear RED in solidarity https://www.bustle.com/p/the-day-without-a-woman-strike-det…\n",
      "\n",
      "https://www.flippable.org/blog/2017/2/…/the-unseen-elections\n",
      "\n",
      "Get the real story on health care and the ACA\n",
      "\n",
      "If we don't have a planet, the rest of this doesn't matter!! Friends: the House GOP has filed a bill to eliminate the Environmental Protection Agency, effective December 2018. The EPA performs critical functions to protect human and environmental public health. I know it's tempting to be focused on Flynn, intelligence leaks, and Cabinet appointees at this time, but we should remain vigilant about substantive things on the GOP agenda. If you want to protect the EPA, here are 3... things you can do: 1. If you live in Florida's first Congressional district, call your Congressman (Matt Gaetz), who sponsored the legislation and tell him you oppose it. 2. Contact your own member of the House of Representatives to voice your opposition. 3. The bill has been referred to the House Committee on Space, Science, and Technology. Consider contacting the Chairman of that committee (Lamar Smith, TX) or the ranking member (Eddie Johnson, TX) to register your opposition. 4. Check the membership roster of the Committee to see if your rep serves on that committee, and if so, contact him/her to oppose it. The Democrats on that Committee are Johnson (TX-30th); Lofgren (CA-19th); Lipinski (IL-3rd); Bonamici (OR-1st); Bera (CA-7th); Esty (CT-5th); Veasey (TX-33rd); Beyer (VA-8th) and Rosen (NV-3rd). Please copy and repost, don't just click Share.\n",
      "\n",
      "Check out our new web page for calendar of events. https://www.daytonwomensrightsalliance.com/\n",
      "\n",
      "This is soooo wrong. https://thinkprogress.org/republicans-trump-hillary-emails-…\n",
      "\n",
      "URGENT- Call Senator Rob Portman today-- the Senate is voting on Pruitt for EPA -- a man who repeatedly sued the EPA on behalf of oil & gas industry. Ask Portman to at least request a delay in confirmation of Pruitt until after records of Pruitt's contacts with oil & gas industry are released by court order next Tuesday. Call 513-684-3265 or 800-205-ohio or 419-259-3895 or 202-224-3353 or 216-522-7095 http://www.chicagotribune.com/…/ct-scott-pruitt-epa-emails-…\n",
      "\n",
      "WHERE IS REPRESENTATIVE MIKE TURNER? We'd like to meet with you, Mike -- We will be outside your Dayton officeTuesday,  Feb 21 at 10:00am to ask you to meet with us --there are a few issues we'd like to discuss!  Anyone who'd like to have a Town Hall meeting with Mike should join us at his office 120 West 3rd S 2/21 -- bring signs with your questions and binoculars so we can try to find Turner-- https://www.facebook.com/events/230139680789801/\n",
      "\n",
      "Stop the Attack on Title X https://www.istandwithpp.org/call/titlex/house\n",
      "\n",
      "It takes a man's signature.\n",
      "\n",
      "Information saying this event was full was wrong-- you can still sign up for this -- just click on link-- hope to see you there!\n",
      "\n",
      "Good coverage of our Rally to Support Planned Parenthood today by Fox45. Disappointed WHIO channel 7 and WDTN channel 2 only covered the much smaller group attacking Planned Parenthood, even though they were both notified of our rally. http://fox45now.com/…/march-to-show-support-for-planned-par…\n",
      "\n"
     ]
    }
   ],
   "source": [
    "divs = soup.findAll('div',attrs={'class':'userContent'})\n",
    "for div in divs:\n",
    "  ps = div.findAll('p')\n",
    "  for p in ps:\n",
    "    print p.text\n",
    "    print\n",
    "#     if p.find('a'):\n",
    "#       print p.find('a')['href']\n",
    "#       print\n",
    "#       if p.find('a',attrs={'class':'_5inf'}):\n",
    "#         print p.find('a',attrs={'class':'_5inf'})['href']\n",
    "#         print p.find('a',attrs={'class':'_5inf'})['title']\n",
    "#       print\n",
    "#       print\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'protest AND \"Nashville, TN\" AND \"facebook event\"'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sources = ['facebook','change.org']\n",
    "' AND '.join(keyword_searches[0]) + ' AND \\\"facebook event\\\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error accessing: http://www.google.com/search?q=%22Scottsdale+%2C+AZ%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=0&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Scottsdale+%2C+AZ%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=10&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Norfolk+%2C+VA%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=0&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Norfolk+%2C+VA%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=10&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Madison+%2C+WI%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=0&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Madison+%2C+WI%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=10&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Orlando+%2C+FL%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=0&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Orlando+%2C+FL%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=10&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Birmingham+%2C+AL%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=0&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Birmingham+%2C+AL%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=10&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Baton+Rouge+%2C+LA%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=0&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Baton+Rouge+%2C+LA%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=10&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Durham+%2C+NC%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=0&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Durham+%2C+NC%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=10&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Laredo+%2C+TX%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=0&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Laredo+%2C+TX%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=10&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Lubbock+%2C+TX%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=0&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Lubbock+%2C+TX%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=10&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Chesapeake+%2C+VA%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=0&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Chesapeake+%2C+VA%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=10&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Chula+Vista+%2C+CA%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=0&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Chula+Vista+%2C+CA%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=10&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Garland+%2C+TX%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=0&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Garland+%2C+TX%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=10&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Winston-Salem+%2C+NC%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=0&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22Winston-Salem+%2C+NC%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=10&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22North+Las+Vegas+%2C+NV%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=0&num=10&nl=en\n",
      "Error accessing: http://www.google.com/search?q=%22North+Las+Vegas+%2C+NV%22+AND+%28protest+OR+rally+OR+demonstration%29+AND+%22facebook+event%22&start=10&num=10&nl=en\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-7eee46e783d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maction_categories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcity_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m78\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0msearch_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' AND ('\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' OR '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_categories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m') AND \\\"facebook event\\\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msearch_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m          \u001b[0;31m### Extract Date from Facebook Event Description if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/modules/standard_search.pyc\u001b[0m in \u001b[0;36msearch\u001b[0;34m(query, pages, lang, void)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_search_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/modules/utils.pyc\u001b[0m in \u001b[0;36mget_html\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 447\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mhttp_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttplib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPConnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m     \u001b[0mhttp_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# buffering kw not supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self, buffering)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwill_close\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_UNKNOWN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CS_IDLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# Initialize with Simple-Response defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "num_page = 2\n",
    "nresults = 0\n",
    "today = datetime.datetime.today()\n",
    "weeks_ago = today - datetime.timedelta(days=14)\n",
    "\n",
    "with open(os.path.join(os.pardir, 'data/documents', 'google_real_data_' + today.strftime(\"%Y%m%d%H%M\") + '.txt'), 'w') as outfile:\n",
    "  outfile.write('[')\n",
    "  for category in action_categories.keys():\n",
    "    for city in city_list[77:]:\n",
    "      search_results = google.search(city + ' AND (' + ' OR '.join(action_categories[category]) + ') AND \\\"facebook event\\\"', num_page)\n",
    "      for result in search_results:\n",
    "         ### Extract Date from Facebook Event Description if it exists\n",
    "        if result.link.startswith('https://www.facebook.com/events/'):\n",
    "          facebook_event_id = result.link.split(\"events/\",1)[1][:-1]  \n",
    "          try:\n",
    "            event_date = dparser.parse(result.description,fuzzy=True).strftime(\"%m/%d/%Y\")\n",
    "          except:\n",
    "            event_date = None\n",
    "\n",
    "          try:\n",
    "            json_doc = {\n",
    "                              'posted_date': datetime.datetime.today().strftime(\"%m/%d/%Y\"),\n",
    "                              'expiration_date': event_date,\n",
    "                              'action_keywords': category, \n",
    "                              'issues_keywords': None, \n",
    "                              'document': {'content': result.description,\n",
    "                                         'title': result.name,\n",
    "                                         'summary': None,\n",
    "                                         'url': result.link,\n",
    "                                         'date': event_date,\n",
    "                                      },\n",
    "                              'location': {'addr1': None,\n",
    "                                         'addr2': None,\n",
    "                                         'city':  city.split(',',1)[0],\n",
    "                                         'state': city.split(',',1)[1],\n",
    "                                         'zip':  None ,\n",
    "                                         'lat':   None,\n",
    "                                         'long':  None,\n",
    "                                         },\n",
    "                              'org_name': None,\n",
    "\n",
    "                              # your mileage may vary\n",
    "                              'other_metadata': {\n",
    "                                  'google_cached_page': result.cached,\n",
    "                                  'google_search_link': result.google_link\n",
    "                                  }\n",
    "                          }\n",
    "\n",
    "            new_record = {\n",
    "                                  \"_index\" : \"facebook\",\n",
    "                                  \"_type\"  : \"event\",\n",
    "                                  \"_id\"    : facebook_event_id,\n",
    "                                  \"_source\": json_doc,\n",
    "                              }\n",
    "\n",
    "            json.dump(new_record, outfile)\n",
    "            outfile.write(',')\n",
    "            ntweets += 1\n",
    "            if ntweets % 100:\n",
    "              print(result.description)\n",
    "          except error as e:\n",
    "            print e\n",
    "            time.sleep(15*60)\n",
    "\n",
    "        ### Extract Date from Facebook Event Description if it exists\n",
    "        elif result.link.startswith('https://www.facebook.com/'):\n",
    "          facebook_organization = result.description.split(\"|\",1)[0][:-1]\n",
    "          facebook_id = result.link.split(\"facebook.com/\",1)[1][:-1]  \n",
    "\n",
    "\n",
    "          try:\n",
    "            json_doc = {\n",
    "                              'posted_date': datetime.datetime.today().strftime(\"%m/%d/%Y\"),\n",
    "                              'expiration_date': None,\n",
    "                              'action_keywords': category, \n",
    "                              'issues_keywords': None, \n",
    "                              'document': {'content': result.description,\n",
    "                                         'title': result.name,\n",
    "                                         'summary': None,\n",
    "                                         'url': result.link,\n",
    "                                         'date': None\n",
    "                                      },\n",
    "                              'location': {'addr1': None,\n",
    "                                         'addr2': None,\n",
    "                                         'city':  city.split(',',1)[0],\n",
    "                                         'state': city.split(',',1)[1],\n",
    "                                         'zip':  None ,\n",
    "                                         'lat':   None,\n",
    "                                         'long':  None,\n",
    "                                         },\n",
    "                              'org_name': facebook_organization,\n",
    "\n",
    "                              # your mileage may vary\n",
    "                              'other_metadata': {\n",
    "                                  'google_cached_page': result.cached,\n",
    "                                  'google_search_link': result.google_link\n",
    "                                  }\n",
    "                          }\n",
    "\n",
    "            new_record = {\n",
    "                                  \"_index\" : \"facebook\",\n",
    "                                  \"_type\"  : \"group\",\n",
    "                                  \"_id\"    : facebook_id,\n",
    "                                  \"_source\": json_doc,\n",
    "                              }\n",
    "\n",
    "            json.dump(new_record, outfile)\n",
    "            outfile.write(',')\n",
    "            ntweets += 1\n",
    "            if ntweets % 100:\n",
    "              print(result.description)\n",
    "          except error as e:\n",
    "            print e\n",
    "            time.sleep(15*60)\n",
    "\n",
    "        else:\n",
    "          try:\n",
    "            event_date = dparser.parse(result.description,fuzzy=True).strftime(\"%m/%d/%Y\")\n",
    "          except:\n",
    "            event_date = None  \n",
    "\n",
    "          try:\n",
    "            json_doc = {\n",
    "                              'posted_date': datetime.datetime.today().strftime(\"%m/%d/%Y\"),\n",
    "                              'expiration_date': event_date,\n",
    "                              'action_keywords': category, \n",
    "                              'issues_keywords': None, \n",
    "                              'document': {'content': result.description,\n",
    "                                         'title': result.name,\n",
    "                                         'summary': None,\n",
    "                                         'url': result.link,\n",
    "                                         'date': event_date\n",
    "                                      },\n",
    "                              'location': {'addr1': None,\n",
    "                                         'addr2': None,\n",
    "                                         'city':  city.split(',',1)[0],\n",
    "                                         'state': city.split(',',1)[1],\n",
    "                                         'zip':  None ,\n",
    "                                         'lat':   None,\n",
    "                                         'long':  None,\n",
    "                                         },\n",
    "                              'org_name': None,\n",
    "\n",
    "                              # your mileage may vary\n",
    "                              'other_metadata': {\n",
    "                                  'google_cached_page': result.cached,\n",
    "                                  'google_search_link': result.google_link\n",
    "                                  }\n",
    "                          }\n",
    "\n",
    "            new_record = {\n",
    "                                  \"_index\" : \"facebook\",\n",
    "                                  \"_type\"  : \"group\",\n",
    "                                  \"_id\"    : facebook_id,\n",
    "                                  \"_source\": json_doc,\n",
    "                              }\n",
    "\n",
    "            json.dump(new_record, outfile)\n",
    "            outfile.write(',')\n",
    "            ntweets += 1\n",
    "            if ntweets % 100:\n",
    "              print(result.description)\n",
    "          except error as e:\n",
    "            print e\n",
    "            time.sleep(15*60)\n",
    "\n",
    "  outfile.seek(-1, os.SEEK_END)\n",
    "  outfile.truncate()\n",
    "  outfile.write(']')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:2: DeprecationWarning: You passed a bytestring as `filenames`. This will not work on Python 3. Use `cp.read_file()` or switch to using Unicode strings across the board.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.join(os.pardir, 'config', 'ross.ini'))\n",
    "\n",
    "consumer_key = config['twitter.api']['consumer_key']\n",
    "consumer_secret = config['twitter.api']['consumer_secret']\n",
    "access_token = config['twitter.api']['access_token']\n",
    "access_token_secret = config['twitter.api']['access_token_secret']\n",
    "\n",
    "AWS_ACCESS_KEY_ID = config['aws.creds']['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = config['aws.creds']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "YOUR_API_KEY = config['google.api']['api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(sys)  \n",
    "sys.setdefaultencoding('UTF8')\n",
    "\n",
    "\n",
    "zip_geo = pd.read_csv('~/w210_imwithdata/data/static_data/Zip_lat_long.csv')\n",
    "\n",
    "zips = []\n",
    "\n",
    "for index, row in zip_geo.iterrows():\n",
    "    new_record = {\n",
    "        'zip': row['ZIP'],\n",
    "        'lat': row['LAT'],\n",
    "        'lng': row['LNG']\n",
    "    }\n",
    "    zips.append(new_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lat': 18.180554999999998, 'lng': -66.749960999999999, 'zip': 601.0},\n",
       " {'lat': 18.361945000000002, 'lng': -67.175596999999996, 'zip': 602.0},\n",
       " {'lat': 18.455182999999998, 'lng': -67.119887000000006, 'zip': 603.0},\n",
       " {'lat': 18.158345000000001, 'lng': -66.932911000000004, 'zip': 606.0},\n",
       " {'lat': 18.295366000000001, 'lng': -67.125135, 'zip': 610.0},\n",
       " {'lat': 18.402252999999998, 'lng': -66.711396999999991, 'zip': 612.0},\n",
       " {'lat': 18.420411999999999, 'lng': -66.671979000000007, 'zip': 616.0},\n",
       " {'lat': 18.445146999999999, 'lng': -66.559696000000002, 'zip': 617.0},\n",
       " {'lat': 17.991245000000003, 'lng': -67.153993, 'zip': 622.0},\n",
       " {'lat': 18.083361, 'lng': -67.153897000000001, 'zip': 623.0}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zips[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GoogPlac(lat,lng,keyword,radius,language,pagetoken,key):\n",
    "  #making the url\n",
    "    if pagetoken is str:\n",
    "        AUTH_KEY = key\n",
    "        LOCATION = str(lat)+ \",\" + str(lng)\n",
    "        KEYWORD =keyword\n",
    "        RADIUS = radius\n",
    "        LANGUAGE = language\n",
    "        PAGETOKEN = pagetoken\n",
    "        MyUrl = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json'\n",
    "                '?location=%s'\n",
    "                    '&radius=%s'\n",
    "                    '&keyword=%s'\n",
    "                '&language=%s'\n",
    "                '&page_token=%s'\n",
    "                '&sensor=false&key=%s') % (LOCATION, RADIUS, KEYWORD, LANGUAGE,PAGETOKEN, AUTH_KEY)\n",
    "        #grabbing the JSON result\n",
    "    else:\n",
    "        AUTH_KEY = key\n",
    "        LOCATION = str(lat)+ \",\" + str(lng)\n",
    "        KEYWORD =keyword\n",
    "        RADIUS = radius\n",
    "        LANGUAGE = language\n",
    "        MyUrl = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json'\n",
    "                '?location=%s'\n",
    "                    '&radius=%s'\n",
    "                    '&keyword=%s'\n",
    "                '&language=%s'\n",
    "                '&sensor=false&key=%s') % (LOCATION, RADIUS, KEYWORD, LANGUAGE, AUTH_KEY)\n",
    "        #grabbing the JSON result\n",
    "        response = urllib.urlopen(MyUrl)\n",
    "        jsonRaw = response.read()\n",
    "        jsonData = json.loads(jsonRaw)\n",
    "        return jsonData\n",
    "\n",
    "\n",
    "def GoogDetails(place_id,key):\n",
    "  #making the url\n",
    "  AUTH_KEY = key\n",
    "  PLACEID = place_id\n",
    "  MyUrl = ('https://maps.googleapis.com/maps/api/place/details/json'\n",
    "           '?placeid=%s'\n",
    "           '&sensor=false&key=%s') % (PLACEID, AUTH_KEY)\n",
    "  #grabbing the JSON result\n",
    "  response = urllib.urlopen(MyUrl)\n",
    "  jsonRaw = response.read()\n",
    "  jsonData = json.loads(jsonRaw)\n",
    "  return jsonData\n",
    "\n",
    "google_place_ids = []\n",
    "\n",
    "i = 0\n",
    "with open('data_google_places1.txt', 'a') as outfile:\n",
    "  for search in keyword_searches:\n",
    "      for i in xrange(0,len(zips),30):\n",
    "  #         if zips[i]['zip'] in range(60600,60702):\n",
    "              #if i >= 434:\n",
    "              # Retry Query for a Max of 3 pages, depending on whether the query has a next_page_token\n",
    "            for k in range(3):\n",
    "                # First page will not include a next_page_token in the query\n",
    "                if k == 0:\n",
    "                    # Grab the JSON result\n",
    "                    content = GoogPlac(zips[i]['lat'],zips[i]['lng'],' '.join(search),'50000','en',False,YOUR_API_KEY)\n",
    "                    # Check for a next page token\n",
    "                    next_page_token = content.get('next_page_token',None)\n",
    "                    # Loop through the places and get the details necessary to populate the final records for Placement\n",
    "                    for place in content['results']:\n",
    "                        # Query Google Details with the Place ID\n",
    "                        details = GoogDetails(place['place_id'],YOUR_API_KEY)\n",
    "                        new_record = {\n",
    "                                            'source': 'Google',\n",
    "                                            'keywords': search,\n",
    "                                            'name': details['result'].get('name','None'),\n",
    "                                            'google_place_id': place['place_id'],\n",
    "                                            'address': details['result'].get('formatted_address','None'),\n",
    "                                            'phone': details['result'].get('formatted_phone_number','None'),\n",
    "                                            'latitude': place['geometry']['location']['lat'],\n",
    "                                            'longitude': place['geometry']['location']['lng'],\n",
    "                                            'google_url': details['result'].get('url','None'),\n",
    "                                            'website':details['result'].get('website','None'),\n",
    "                                            }\n",
    "                        if place['place_id'] not in google_place_ids:\n",
    "                        # Append the record to the output list\n",
    "                          json.dump(new_record, outfile)\n",
    "                          outfile.write(',')\n",
    "                          outfile.write('\\n')\n",
    "                          google_place_ids.append(place['place_id'])\n",
    "                        # Keep track of the output\n",
    "                        if len(google_place_ids) % 100 == 0:\n",
    "                            print len(google_place_ids)\n",
    "                            print new_record\n",
    "                # If the original query has a next page token, then query the next page\n",
    "                if k > 0 and next_page_token: \n",
    "                    # Add the next page token to the query\n",
    "                    content = GoogPlac(zips[i]['lat'],zips[i]['lng'],' '.join(search),'50000','en',next_page_token,YOUR_API_KEY)\n",
    "                    # If there's content, then get the new records\n",
    "                    if content:\n",
    "                        next_page_token = content.get('next_page_token',None)\n",
    "                        # Checking that this part of the code was actually working, so  print page # if afte first page\n",
    "                        # Google only lets you get to 60 results per query, or 3 pages, so it will never be > 3\n",
    "                        print 'in page %s of results' % (k+1)\n",
    "                        for place in content['results']:\n",
    "                            # Query Google Details with the Place ID\n",
    "                            details = GoogDetails(place['place_id'],YOUR_API_KEY)\n",
    "                            new_record = {\n",
    "                                                'source': 'Google',\n",
    "                                                'keywords': search,\n",
    "                                                ''\n",
    "                                                'name': details['result'].get('name','None'),\n",
    "                                                'google_place_id': place['place_id'],\n",
    "                                                'address': details['result'].get('formatted_address','None'),\n",
    "                                                'phone': details['result'].get('formatted_phone_number','None'),\n",
    "                                                'latitude': place['geometry']['location']['lat'],\n",
    "                                                'longitude': place['geometry']['location']['lng'],\n",
    "                                                'google_url': details['result'].get('url','None'),\n",
    "                                                'website':details['result'].get('website','None'),\n",
    "                                                }\n",
    "                            # Append the record to the output list\n",
    "                            if place['place_id'] not in google_place_ids:\n",
    "                            # Append the record to the output list\n",
    "                              json.dump(new_record, outfile)\n",
    "                              outfile.write(',')\n",
    "                              outfile.write('\\n')\n",
    "                              google_place_ids.append(place['place_id'])\n",
    "                            # Keep track of the output\n",
    "                            if len(google_place_ids) % 100 == 0:\n",
    "                                print len(google_place_ids)\n",
    "                                print new_record\n",
    "                    else:\n",
    "                        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bucket_name = 'mids-capstone-rzst'\n",
    "# conn = boto.connect_s3(AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "# def percent_cb(complete, total):\n",
    "#     sys.stdout.write('.')\n",
    "#     sys.stdout.flush()\n",
    "\n",
    "# bucket = conn.get_bucket('mids-capstone-rzst')\n",
    "# key = boto.s3.key.Key(bucket, os.path.join(os.pardir,\"data_google_places1.txt\"))\n",
    "# key.set_contents_from_filename(os.path.join(os.pardir,\"data_google_places1.txt\"))\n",
    "# key.set_acl('public-read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.pardir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bucket_name = 'mids-capstone-rzst'\n",
    "# conn = boto.connect_s3(AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "\n",
    "# # bucket = conn.create_bucket(bucket_name,\n",
    "# #     location=boto.s3.connection.Location.DEFAULT)\n",
    "\n",
    "# testfile = os.path.join(os.pardir,\"data4.txt\")\n",
    "# print 'Uploading %s to Amazon S3 bucket %s' % \\\n",
    "#    (testfile, bucket_name)\n",
    "\n",
    "# def percent_cb(complete, total):\n",
    "#     sys.stdout.write('.')\n",
    "#     sys.stdout.flush()\n",
    "\n",
    "\n",
    "# # k = Key(bucket_name)\n",
    "# # k.key = 'data.txt'\n",
    "# # k.set_contents_from_filename(testfile,\n",
    "# #     cb=percent_cb, num_cb=10)\n",
    "\n",
    "\n",
    "# # s3_connection = boto.connect_s3()\n",
    "# bucket = conn.get_bucket('mids-capstone-rzst')\n",
    "# key = boto.s3.key.Key(bucket, os.path.join(os.pardir,\"data4.txt\"))\n",
    "# key.set_contents_from_filename(os.path.join(os.pardir,\"data4.txt\"))\n",
    "# key.set_acl('public-read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filein = os.path.join(os.pardir,\"data_google_places1.txt\") \n",
    "\n",
    "# f = open(filein,'r')\n",
    "# filedata = f.read()\n",
    "# f.close()\n",
    "\n",
    "# newdata = filedata.replace(\"\\n\",\",\")\n",
    "\n",
    "# fileout = os.path.join(os.pardir,\"data_google_places2.json\") \n",
    "\n",
    "# f = open(fileout,'w+')\n",
    "# f.write('[')\n",
    "# f.write(newdata)\n",
    "# f.seek(-1, os.SEEK_END)\n",
    "# f.truncate()\n",
    "# f.write(']')\n",
    "# f.close()\n",
    "\n",
    "\n",
    "# d = simplejson.loads(open(os.path.join(os.pardir,\"data_google_places2.json\")).read())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address': 'Alden St, Springfield, MA 01109, USA',\n",
       " 'google_place_id': 'ChIJZ_alNrLn5okRbrIugLIZnQo',\n",
       " 'google_url': 'https://maps.google.com/?cid=764795766168269422',\n",
       " 'keywords': ['donation', 'civil rights'],\n",
       " 'latitude': 42.10410779999999,\n",
       " 'longitude': -72.5534139,\n",
       " 'name': 'NAACP Springfield',\n",
       " 'phone': 'None',\n",
       " 'source': 'Google',\n",
       " 'website': 'None'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
