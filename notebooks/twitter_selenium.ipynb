{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from pyvirtualdisplay import Display\n",
    "import gnp\n",
    "import codecs\n",
    "import json\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.key import Key\n",
    "import boto\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "from TwitterSearch import *\n",
    "from sunlight import congress\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import itertools\n",
    "import datetime\n",
    "import configparser\n",
    "import time\n",
    "from pattern.en import parsetree\n",
    "from pattern.en import mood\n",
    "import dateutil.parser as dparser\n",
    "import sunlight\n",
    "from sunlight import congress\n",
    "import re\n",
    "from postal.parser import parse_address\n",
    "import csv\n",
    "import selenium\n",
    "from bs4 import BeautifulSoup as soupy\n",
    "import tweepy\n",
    "import codecs\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action_categories = {'donation': [\"donation\",\"donate\",\"contribute\"],\n",
    "                     'protest': [\"protest\",\"rally\",\"demonstration\"],\n",
    "                     'indivisible': [\"meeting\",\"gathering\",\"indivisible\"],\n",
    "                     'boycott': [\"boycott\"],\n",
    "                     'advocate': [\"contact your representative\",\"contact your senator\",\"petition\",\n",
    "                                 \"call your representative\",\"call your senator\",\n",
    "                                 \"email your representative\",\"email your senator\"],\n",
    "#                      'vote': [\"vote\",\"cast your ballot\"]\n",
    "#                      'town hall': [\"town hall\",\"open office\",\"town meeting\",\"townhall\",'virtual townhall','listening session']\n",
    "                          }\n",
    "\n",
    "# action_categories = {'donation': [\"donate\"],\n",
    "#                      'protest': [\"protest\"],\n",
    "#                      'gathering': [\"indivisible\"],\n",
    "#                      'boycott': [\"boycott\"],\n",
    "#                      'advocate': [\"representative\",\"senator\"],\n",
    "#                      'vote': [\"vote\",\"cast your ballot\"],\n",
    "#                      'town hall': [\"town hall\",]\n",
    "#                           }\n",
    "\n",
    "issue_types = { 'immigrant rights' : ['immigrant rights',\"immigrants' rights\",'refugee rights', 'travel ban',\n",
    "                                        'border wall','refugees','asylum','immigration reform',\n",
    "                                        'immigrant advocacy','migrant rights','undocumented'],\n",
    "               \"women's rights\" : [\"women's rights\",\"women's rights\",'womens rights','gender equality',\n",
    "                                  'girl power',\"international women's day\",\"war on women\",\"planned parenthood\"],\n",
    "               \"civil rights\" : [\"racial equality\",\"black lives matter\",\"african american rights\",\"civil rights\",\n",
    "                                \"black power\",\"jim crow\"],\n",
    "               \"LGTBQ rights\" : [\"marriage equality\",\"transgender rights\",\"equality act\",\"lesbian rights\",'gay rights',\n",
    "                                'bisexual rights'],\n",
    "               \"voting rights\": [\"redistricting\",\"gerrymandering\",\"redistrict\",\"gerrymander\",\"voter id\",\"voting access\",\n",
    "                                \"voter access\",\"voter suppression\"],\n",
    "               \"worker rights\": ['worker rights','right to work','minimum wage']\n",
    "                      }\n",
    "\n",
    "# issue_types = { 'immigrant rights' : ['immigrant rights'],\n",
    "#                \"women's rights\" : [\"women's rights\"],\n",
    "#                \"civil rights\" : [\"civil rights\"],\n",
    "#                \"LGTBQ rights\" : ['gay rights','transgender rights'],\n",
    "#                \"voting rights\": [\"redistricting\",\"gerrymandering\",\"voter suppression\"],\n",
    "#                \"worker rights\": ['worker rights']\n",
    "#                                       }\n",
    "\n",
    "# organization_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:2: DeprecationWarning: You passed a bytestring as `filenames`. This will not work on Python 3. Use `cp.read_file()` or switch to using Unicode strings across the board.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.join(os.pardir, 'config', 'ross.ini'))\n",
    "\n",
    "consumer_key = config['twitter.api']['consumer_key']\n",
    "consumer_secret = config['twitter.api']['consumer_secret']\n",
    "access_token = config['twitter.api']['access_token']\n",
    "access_token_secret = config['twitter.api']['access_token_secret']\n",
    "\n",
    "AWS_ACCESS_KEY_ID = config['aws.creds']['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = config['aws.creds']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LGTBQ rights', 'worker rights', 'civil rights', 'immigrant rights', \"women's rights\", 'voting rights']\n"
     ]
    }
   ],
   "source": [
    "action_categories.keys()\n",
    "print issue_types.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'title', u'full_title', u'firstname', u'middlename', u'lastname',\n",
      "       u'name_suffix', u'nickname', u'party', u'state', u'district',\n",
      "       u'in_office', u'gender', u'phone', u'fax', u'website', u'webform',\n",
      "       u'congress_office', u'bioguide_id', u'votesmart_id', u'fec_id',\n",
      "       u'govtrack_id', u'crp_id', u'twitter_id', u'congresspedia_url',\n",
      "       u'youtube_url', u'facebook_id', u'official_rss', u'senate_class',\n",
      "       u'birthdate', u'oc_email', u'title_name'],\n",
      "      dtype='object')\n",
      "\n",
      "['Rep Robert Aderholt', 'Sen Lamar Alexander', 'Rep Justin Amash', 'Rep Mark Amodei', 'Rep Alma Adams', 'Rep Pete Aguilar', 'Rep Rick Allen', 'Rep Ralph Abraham', 'Rep Jodey Arrington', 'Rep Joe Barton']\n"
     ]
    }
   ],
   "source": [
    "legislators = pd.read_csv(os.path.join(os.pardir, 'data/static_data', 'legislators.csv'))\n",
    "\n",
    "mask = (legislators.in_office == 1)\n",
    "\n",
    "legislators = legislators.loc[mask]\n",
    "\n",
    "legislators['title_name'] = legislators['title'] + ' ' + legislators['firstname'] + ' ' + legislators['lastname']\n",
    "\n",
    "print legislators.columns\n",
    "print\n",
    "print list(legislators['title_name'])[:10]\n",
    "legislator_list = list(legislators['title_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                              org_name\n",
      "0           0                       Accord Alliance\n",
      "1           1                                ACT UP\n",
      "2           2         Advocates for Informed Choice\n",
      "3           3    Affirmation: Gay & Lesbian Mormons\n",
      "4           4                      Against Equality\n",
      "5           5  American Foundation for Equal Rights\n",
      "6           6    American Veterans for Equal Rights\n",
      "7           7                          Athlete Ally\n",
      "8           8                        Atticus Circle\n",
      "9           9                            Bash Back!\n"
     ]
    }
   ],
   "source": [
    "immigration_orgs = pd.read_csv(os.path.join(os.pardir, 'data/static_data', 'immigration_orgs.csv'))\n",
    "civil_rights_orgs = pd.read_csv(os.path.join(os.pardir, 'data/static_data', 'civil_rights_orgs.csv'))\n",
    "lgbtq_orgs = pd.read_csv(os.path.join(os.pardir, 'data/static_data', 'lgbtq_orgs.csv'))\n",
    "\n",
    "print lgbtq_orgs[:10]\n",
    "\n",
    "# for org in immigration_orgs['org_name']:\n",
    "#   print org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            city state population            full_city\n",
      "0      New York     NY  8,363,710      \"New York , NY\"\n",
      "1   Los Angeles     CA  3,833,995   \"Los Angeles , CA\"\n",
      "2       Chicago     IL  2,853,114       \"Chicago , IL\"\n",
      "3       Houston     TX  2,242,193       \"Houston , TX\"\n",
      "4       Phoenix     AZ  1,567,924       \"Phoenix , AZ\"\n",
      "5  Philadelphia     PA  1,447,395  \"Philadelphia , PA\"\n",
      "6   San Antonio     TX  1,351,305   \"San Antonio , TX\"\n",
      "7        Dallas     TX  1,279,910        \"Dallas , TX\"\n",
      "8     San Diego     CA  1,279,329     \"San Diego , CA\"\n",
      "9      San Jose     CA    948,279      \"San Jose , CA\"\n",
      "['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', 'San Antonio', 'Dallas', 'San Diego', 'San Jose']\n"
     ]
    }
   ],
   "source": [
    "cities = pd.read_csv(os.path.join(os.pardir, 'data/static_data', 'Top5000Population.csv'))\n",
    "cities['full_city'] = '\\\"' + cities['city'] + ', ' + cities['state'] + '\\\"'\n",
    "print cities[:10]\n",
    "city_list = list(cities['city'].str.rstrip())\n",
    "state_list = list(cities['state'].str.rstrip())\n",
    "# city_tuples = [(city, state) for city in city_list]\n",
    "print city_list[:10]\n",
    "# print city_list.index('Jersey City, NJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220047\n",
      "[u'Rep Lee Zeldin meeting', u'Rep Lee Zeldin gathering', u'Rep Lee Zeldin indivisible', u'Rep Ryan Zinke contact your representative', u'Rep Ryan Zinke contact your senator', u'Rep Ryan Zinke petition', u'Rep Ryan Zinke call your representative', u'Rep Ryan Zinke call your senator', u'Rep Ryan Zinke email your representative', u'Rep Ryan Zinke email your senator', u'Rep Ryan Zinke donation', u'Rep Ryan Zinke donate', u'Rep Ryan Zinke contribute', u'Rep Ryan Zinke boycott', u'Rep Ryan Zinke protest', u'Rep Ryan Zinke rally', u'Rep Ryan Zinke demonstration', u'Rep Ryan Zinke meeting', u'Rep Ryan Zinke gathering', u'Rep Ryan Zinke indivisible']\n"
     ]
    }
   ],
   "source": [
    "keyword_searches = []\n",
    "### Come up wiht long list of queries\n",
    "for issue in issue_types.keys():\n",
    "  for issue_keyword in issue_types[issue]:\n",
    "    for combination in itertools.product([issue_keyword],city_list):\n",
    "      keyword_searches.append(' '.join(list(combination)).decode('utf-8','ignore'))\n",
    "      \n",
    "for issue in issue_types.keys():\n",
    "  for issue_keyword in issue_types[issue]:\n",
    "    for category in action_categories.keys():\n",
    "      for action_keyword in action_categories[category]:\n",
    "        combination = [action_keyword,issue_keyword]\n",
    "        keyword_searches.append(' '.join(combination).decode('utf-8','ignore'))\n",
    "\n",
    "for org in immigration_orgs:\n",
    "  for category in action_categories.keys():\n",
    "    for action_keyword in action_categories[category]: \n",
    "      combination = [action_keyword,org]\n",
    "      keyword_searches.append(' '.join(combination).decode('utf-8','ignore'))\n",
    "\n",
    "for org in civil_rights_orgs:\n",
    "  for category in action_categories.keys():\n",
    "    for action_keyword in action_categories[category]: \n",
    "      combination = [action_keyword,org]\n",
    "      keyword_searches.append(' '.join(combination).decode('utf-8','ignore'))\n",
    "          \n",
    "for org in lgbtq_orgs:\n",
    "  for category in action_categories.keys():\n",
    "    for action_keyword in action_categories[category]: \n",
    "      combination = [action_keyword,org]\n",
    "      keyword_searches.append(' '.join(combination).decode('utf-8','ignore'))\n",
    "          \n",
    "for leg in legislator_list:\n",
    "  for category in action_categories.keys():\n",
    "    for category_keyword in action_categories[category]:\n",
    "      combination = [leg,category_keyword]\n",
    "      keyword_searches.append(' '.join(combination).decode('utf-8','ignore'))\n",
    "\n",
    "print len(keyword_searches)\n",
    "print keyword_searches[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# today = datetime.datetime.today()\n",
    "# with io.open('keyword_searches_shortest.txt', 'wb') as f:\n",
    "#   for s in keyword_searches:\n",
    "#     f.write(('\"' + s.encode('ascii','ignore').rstrip() + '\"' u','))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(800, 600))\n",
    "display.start()\n",
    "browser = webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "today = datetime.datetime.today()\n",
    "weeks_ago = today - datetime.timedelta(days=5)\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth_handler=auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "\n",
    "def tweepy_this_id(tweet_id, action_keywords, issue_keywords, search_city):  \n",
    "  tweet_status = api.get_status(int(tweet_id))\n",
    "  tweet = tweet_status._json\n",
    "  try:\n",
    "    ### Extract URL from Tweet if it exists\n",
    "    try:\n",
    "      if tweet.get('entities',False).get('media',False).get('urls',False):\n",
    "        tweet_urls = tweet['entities']['media']['urls']\n",
    "      else:\n",
    "        tweet_urls = ''\n",
    "    except:\n",
    "      tweet_urls = ''\n",
    "\n",
    "  ### Extract Date from Tweet if it exists\n",
    "    try:\n",
    "      tweet_date = dparser.parse(tweet['text'],fuzzy=True).strftime(\"%m/%d/%Y\")\n",
    "    except:\n",
    "      tweet_date = None\n",
    "  ### Extract Address from Tweet if it exists\n",
    "    try:\n",
    "      address_components = {}\n",
    "      for each in parse_address(tweet['text']):\n",
    "        address_components[each[1]] = each[0]\n",
    "      address = address_components.get('house_number','') + ' ' + address_components.get('road','')\n",
    "      if search_city:\n",
    "        city = search_city\n",
    "      else:\n",
    "        city = address_components.get('city',None)\n",
    "      state = address_components.get('state',None)\n",
    "      zipcode = address_components.get('postcode',None)\n",
    "    except:\n",
    "      address = ''\n",
    "      city = None\n",
    "      state = None\n",
    "      zipcode = None\n",
    "  ### Extract City and State from User Location\n",
    "    try:\n",
    "      if city == None:\n",
    "        matches = re.findall(\"([\\w\\s]+),\\s(\\w+)\", string)\n",
    "        match = matches[0]\n",
    "        tweet_city = match[0]\n",
    "      else:\n",
    "        tweet_city == city\n",
    "      if state == None:\n",
    "        matches = re.findall(\"([\\w\\s]+),\\s(\\w+)\", string)\n",
    "        match = matches[0]\n",
    "        tweet_state = match[1]\n",
    "      else:\n",
    "        tweet_state = state\n",
    "    except:\n",
    "      tweet_city = ''\n",
    "      tweet_state = ''\n",
    "    ### Get Hashtags\n",
    "    try:\n",
    "      if tweet.get('entities',False).get('hashtags',False):\n",
    "        tweet_hashtags = tweet['entities']['hashtags']\n",
    "      else:\n",
    "        tweet_hashtags = []\n",
    "    except:\n",
    "      tweet_hashtags = []\n",
    "    ### Get User Mentions\n",
    "    try:\n",
    "      if tweet.get('entities',False).get('user_mentions',False):\n",
    "        tweet_users = tweet['entities']['user_mentions']\n",
    "      else:\n",
    "        tweet_users = []\n",
    "    except:\n",
    "      tweet_users = []\n",
    "\n",
    "    json_doc = {\n",
    "                      'posted_date': datetime.datetime.today().strftime(\"%m/%d/%Y\"),\n",
    "                      'expiration_date': tweet_date,\n",
    "                      'action_keywords': action_keywords, \n",
    "                      'issues_keywords': issue_keywords, \n",
    "                      'document': {'content': tweet['text'],\n",
    "                                 'title': None,\n",
    "                                 'summary': None,\n",
    "                                 'url': 'https://twitter.com/statuses/' + str(tweet['id']),\n",
    "                                 'date': tweet['created_at']\n",
    "                              },\n",
    "                      'location': {'addr1': address,\n",
    "                                 'addr2': None,\n",
    "                                 'city':  tweet_city,\n",
    "                                 'state': tweet_state,\n",
    "                                 'zip':  zipcode ,\n",
    "                                 'lat':   None,\n",
    "                                 'long':  None,\n",
    "                                 },\n",
    "                      'org_name': org,\n",
    "\n",
    "                      # your mileage may vary\n",
    "                      'other_metadata': {\n",
    "                          'twitter_coordinates': tweet['coordinates'],\n",
    "                          'twitter_urls': tweet_urls,\n",
    "                          'twitter_user_mentions': tweet_users,\n",
    "                          'twitter_hashtags': tweet_hashtags,\n",
    "                          'twitter_favorite_count': tweet['favorite_count'],\n",
    "                          'twitter_retweet_count': tweet['retweet_count']\n",
    "                          }\n",
    "                  }\n",
    "\n",
    "    new_record = {\n",
    "                          \"_index\" : \"twitter\",\n",
    "                          \"_type\"  : \"tweet\",\n",
    "                          \"_id\"    : tweet['id'],\n",
    "                          \"_source\": json_doc,\n",
    "                      }\n",
    "\n",
    "\n",
    "    return new_record\n",
    "  \n",
    "  except Exception, e: # take care of all those ugly errors if there are some\n",
    "    print(e)\n",
    "    print tweet_id\n",
    "    time.sleep(60)\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "today = datetime.datetime.today()\n",
    "weeks_ago = today - datetime.timedelta(days=3)\n",
    "\n",
    "# tweet_users = []\n",
    "# tweet_timestamps = []\n",
    "# tweet_actions = []\n",
    "# tweet_issues= []\n",
    "\n",
    "# for org in immigration_orgs['org_name']:\n",
    "\n",
    "for city in city_list[5:]:\n",
    "  with open(os.path.join(os.pardir, 'data/documents', 'tweepy_real_data_' + today.strftime(\"%Y%m%d%H\") + '.txt'), 'a+') as outfile:\n",
    "    for issue in issue_types:\n",
    "      for issue_keyword in issue:\n",
    "        for category in action_categories.keys():\n",
    "\n",
    "            display = Display(visible=0, size=(800, 600))\n",
    "            display.start()\n",
    "            browser = webdriver.Firefox()\n",
    "\n",
    "            tweet_ids = []\n",
    "            tweet_texts = []\n",
    "\n",
    "            action_list = action_categories[category]\n",
    "            issue_list = issue_types[issue]\n",
    "\n",
    "            tso = TwitterSearchOrder() # create a TwitterSearchOrder object\n",
    "            tso.set_keywords(action_list, or_operator = True)\n",
    "            tso.add_keyword('AND ' + issue_keyword)\n",
    "            tso.add_keyword('AND ' + city)\n",
    "            tso.set_language('en') # we want to see German tweets only\n",
    "            tso.set_include_entities(True) # and don't give us all those entity information\n",
    "            tso.set_result_type('recent')\n",
    "            tso.set_count(30)\n",
    "            querystr = tso.create_search_url() # create the url form the TwitterSearch package\n",
    "            queryURL = 'https://twitter.com/search?l=&'+ querystr[1:] + '+since:'+ weeks_ago.strftime(\"%Y-%m-%d\")\n",
    "            # Use selenium to get the URL\n",
    "            browser.get(queryURL)\n",
    "\n",
    "#               # scroll to the bottom\n",
    "            browser.execute_script(\"window.scrollTo(0, 1000000)\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "            # grab the html\n",
    "            html = browser.page_source\n",
    "            # use bs4 on the html\n",
    "            soup = soupy(html)\n",
    "\n",
    "            tweets = soup.find_all('li','js-stream-item')\n",
    "            for tweet in tweets:\n",
    "              if tweet.find('p','tweet-text'):\n",
    "      #           tweet_user = tweet.find('span','username').text\n",
    "                tweet_text = tweet.find('p','tweet-text').text.encode('utf8')\n",
    "                tweet_id = tweet['data-item-id']\n",
    "      #           timestamp = tweet.find('a','tweet-timestamp')['title']\n",
    "      #           tweet_timestamp = datetime.datetime.strptime(timestamp, '%I:%M %p - %d %b %Y')\n",
    "\n",
    "                tweet_ids.append(tweet_id)\n",
    "                tweet_texts.append(tweet_text)\n",
    "      #           tweet_users.append(tweet_user)\n",
    "      #           tweet_timestamps.append(tweet_timestamp)\n",
    "      #           tweet_actions.append(action_list)\n",
    "      #           tweet_issues.append(issue_list)\n",
    "\n",
    "\n",
    "\n",
    "            tweets_df = pd.DataFrame(\n",
    "                {'tweet_ids': tweet_ids,\n",
    "                 'tweet_texts': tweet_texts\n",
    "                })\n",
    "\n",
    "            unique_tweets = tweets_df.drop_duplicates(subset=['tweet_texts'])\n",
    "\n",
    "            tweet_id_list = list(set(list(unique_tweets['tweet_ids'])))\n",
    "\n",
    "            if len(unique_tweets) > 0:\n",
    "              try:\n",
    "                for i,tweet_id in enumerate(tweet_id_list):\n",
    "                  tweet_json = tweepy_this_id(tweet_id,action_list,issue_list, city)\n",
    "                  if tweet_json:\n",
    "                    json.dump(tweet_json, outfile)\n",
    "                    outfile.write(',')\n",
    "              except:\n",
    "                print 'passed'\n",
    "    #             print tweet_id\n",
    "                pass\n",
    "    #           if i % 5:\n",
    "    #             print tweet_text\n",
    "              time.sleep(5) # pause/sleeps for 5 seconds\n",
    "              browser.quit()\n",
    "              display.stop()\n",
    "            else:\n",
    "  #             print category + ' ' + city\n",
    "  #             print 'no tweets to print'\n",
    "  #             time.sleep(5) # pause/sleeps for 5 seconds\n",
    "              browser.quit()\n",
    "              display.stop()\n",
    "              pass\n",
    "\n",
    "#     outfile.seek(-1, os.SEEK_END)\n",
    "#     outfile.truncate()\n",
    "#     outfile.write(']')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "today = datetime.datetime.today()\n",
    "weeks_ago = today - datetime.timedelta(days=3)\n",
    "\n",
    "# tweet_users = []\n",
    "# tweet_timestamps = []\n",
    "# tweet_actions = []\n",
    "# tweet_issues= []\n",
    "\n",
    "# for org in immigration_orgs['org_name']:\n",
    "\n",
    "for keyword in keyword_searches:\n",
    "  with open(os.path.join(os.pardir, 'data/documents', 'tweepy_real_data_' + today.strftime(\"%Y%m%d%H\") + '.txt'), 'a+') as outfile:\n",
    "\n",
    "    display = Display(visible=0, size=(800, 600))\n",
    "    display.start()\n",
    "    browser = webdriver.Firefox()\n",
    "\n",
    "    tweet_ids = []\n",
    "    tweet_texts = []\n",
    "\n",
    "    action_list = action_categories[category]\n",
    "    issue_list = issue_types[issue]\n",
    "\n",
    "    tso = TwitterSearchOrder() # create a TwitterSearchOrder object\n",
    "    tso.set_keywords(keyword)\n",
    "    tso.set_language('en') # we want to see German tweets only\n",
    "    tso.set_include_entities(True) # and don't give us all those entity information\n",
    "    tso.set_result_type('recent')\n",
    "    tso.set_count(30)\n",
    "    querystr = tso.create_search_url() # create the url form the TwitterSearch package\n",
    "    queryURL = 'https://twitter.com/search?l=&'+ querystr[1:] + '+since:'+ weeks_ago.strftime(\"%Y-%m-%d\")\n",
    "    # Use selenium to get the URL\n",
    "    browser.get(queryURL)\n",
    "    for i in range (2):\n",
    "      # scroll to the bottom\n",
    "      browser.execute_script(\"window.scrollTo(0, 1000000)\")\n",
    "    # grab the html\n",
    "    html = browser.page_source\n",
    "    # use bs4 on the html\n",
    "    soup = soupy(html)\n",
    "\n",
    "    tweets = soup.find_all('li','js-stream-item')\n",
    "    for tweet in tweets:\n",
    "      if tweet.find('p','tweet-text'):\n",
    "#           tweet_user = tweet.find('span','username').text\n",
    "        tweet_text = tweet.find('p','tweet-text').text.encode('utf8')\n",
    "        tweet_id = tweet['data-item-id']\n",
    "#           timestamp = tweet.find('a','tweet-timestamp')['title']\n",
    "#           tweet_timestamp = datetime.datetime.strptime(timestamp, '%I:%M %p - %d %b %Y')\n",
    "\n",
    "        tweet_ids.append(tweet_id)\n",
    "        tweet_texts.append(tweet_text)\n",
    "#           tweet_users.append(tweet_user)\n",
    "#           tweet_timestamps.append(tweet_timestamp)\n",
    "#           tweet_actions.append(action_list)\n",
    "#           tweet_issues.append(issue_list)\n",
    "\n",
    "\n",
    "\n",
    "    tweets_df = pd.DataFrame(\n",
    "        {'tweet_ids': tweet_ids,\n",
    "         'tweet_texts': tweet_texts\n",
    "        })\n",
    "\n",
    "    unique_tweets = tweets_df.drop_duplicates(subset=['tweet_texts'])\n",
    "\n",
    "    tweet_id_list = list(set(list(unique_tweets['tweet_ids'])))\n",
    "\n",
    "    if len(unique_tweets) > 0:\n",
    "      try:\n",
    "        for i,tweet_id in enumerate(tweet_id_list):\n",
    "          tweet_json = tweepy_this_id(tweet_id,keyword,keyword, None)\n",
    "          if tweet_json:\n",
    "            json.dump(tweet_json, outfile)\n",
    "            outfile.write(',')\n",
    "      except:\n",
    "        print 'passed'\n",
    "#             print tweet_id\n",
    "        pass\n",
    "#           if i % 5:\n",
    "#             print tweet_text\n",
    "      time.sleep(5) # pause/sleeps for 10 seconds\n",
    "      browser.quit()\n",
    "    else:\n",
    "#             print category + ' ' + city\n",
    "#             print 'no tweets to print'\n",
    "#             time.sleep(5) # pause/sleeps for 10 seconds\n",
    "      browser.quit()\n",
    "      pass\n",
    "\n",
    "#     outfile.seek(-1, os.SEEK_END)\n",
    "#     outfile.truncate()\n",
    "#     outfile.write(']')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /usr/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c8128995e03d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0mqueryURL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://twitter.com/search?l=&'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mquerystr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'+since:'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mweeks_ago\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;31m# Use selenium to get the URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryURL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# scroll to the bottom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/selenium/webdriver/remote/webdriver.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mweb\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/selenium/webdriver/remote/webdriver.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/selenium/webdriver/remote/remote_connection.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubstitute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/selenium/webdriver/remote/remote_connection.pyc\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparsed_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhttplib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self, buffering)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwill_close\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_UNKNOWN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CS_IDLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# Initialize with Simple-Response defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "today = datetime.datetime.today()\n",
    "weeks_ago = today - datetime.timedelta(days=14)\n",
    "\n",
    "tweet_ids = []\n",
    "tweet_texts = []\n",
    "tweet_users = []\n",
    "tweet_timestamps = []\n",
    "tweet_actions = []\n",
    "tweet_issues= []\n",
    "\n",
    "#### Immigration Orgs\n",
    "for org in immigration_orgs['org_name']:\n",
    "  for issue_keyword in issue_types['immigrant rights']:\n",
    "    for category in action_categories.keys():\n",
    "    \n",
    "      action_list = action_categories[category]\n",
    "      issue_list = issue_types['immigrant rights']\n",
    "\n",
    "      tso = TwitterSearchOrder() # create a TwitterSearchOrder object\n",
    "      tso.set_keywords(action_list, or_operator = True)\n",
    "      tso.add_keyword('AND ' + issue_keyword)\n",
    "      tso.add_keyword('AND ' + org)\n",
    "      tso.set_language('en') \n",
    "      tso.set_include_entities(True) # and don't give us all those entity information\n",
    "      tso.set_result_type('recent')\n",
    "      tso.set_count(20)\n",
    "      querystr = tso.create_search_url() # create the url form the TwitterSearch package\n",
    "      queryURL = 'https://twitter.com/search?l=&'+ querystr[1:] + '+since:'+ weeks_ago.strftime(\"%Y-%m-%d\")\n",
    "      # Use selenium to get the URL\n",
    "      browser.get(queryURL)\n",
    "      for i in range (2):\n",
    "        # scroll to the bottom\n",
    "        browser.execute_script(\"window.scrollTo(0, 1000000)\")\n",
    "      # grab the html\n",
    "      html = browser.page_source\n",
    "      # use bs4 on the html\n",
    "      soup = soupy(html)\n",
    "\n",
    "      tweets = soup.find_all('li','js-stream-item')\n",
    "      for tweet in tweets:\n",
    "        if tweet.find('p','tweet-text'):\n",
    "          tweet_user = tweet.find('span','username').text\n",
    "          tweet_text = tweet.find('p','tweet-text').text.encode('utf8')\n",
    "          tweet_id = tweet['data-item-id']\n",
    "          timestamp = tweet.find('a','tweet-timestamp')['title']\n",
    "          tweet_timestamp = datetime.datetime.strptime(timestamp, '%I:%M %p - %d %b %Y')\n",
    "\n",
    "          tweet_ids.append(tweet_id)\n",
    "          tweet_texts.append(tweet_text)\n",
    "          tweet_users.append(tweet_user)\n",
    "          tweet_timestamps.append(tweet_timestamp)\n",
    "          tweet_actions.append(action_list)\n",
    "          tweet_issues.append(issue_list)\n",
    "        if len(tweet_ids) % 25:\n",
    "          print 'In immigration orgs'\n",
    "          print tweet_text\n",
    "\n",
    "#### Civil Rights Orgs\n",
    "for org in civil_rights_orgs['org_name']:\n",
    "  for issue_keyword in issue_types['civil rights']:\n",
    "    for category in action_categories.keys():\n",
    "    \n",
    "      action_list = action_categories[category]\n",
    "      issue_list = issue_types['civil rights']\n",
    "\n",
    "      tso = TwitterSearchOrder() # create a TwitterSearchOrder object\n",
    "      tso.set_keywords(action_list, or_operator = True)\n",
    "      tso.add_keyword('AND ' + issue_keyword)\n",
    "      tso.add_keyword('AND ' + org)\n",
    "      tso.set_language('en') \n",
    "      tso.set_include_entities(True) # and don't give us all those entity information\n",
    "      tso.set_result_type('recent')\n",
    "      tso.set_count(20)\n",
    "      querystr = tso.create_search_url() # create the url form the TwitterSearch package\n",
    "      queryURL = 'https://twitter.com/search?l=&'+ querystr[1:] + '+since:'+ weeks_ago.strftime(\"%Y-%m-%d\")\n",
    "      # Use selenium to get the URL\n",
    "      browser.get(queryURL)\n",
    "      for i in range (2):\n",
    "        # scroll to the bottom\n",
    "        browser.execute_script(\"window.scrollTo(0, 1000000)\")\n",
    "      # grab the html\n",
    "      html = browser.page_source\n",
    "      # use bs4 on the html\n",
    "      soup = soupy(html)\n",
    "\n",
    "      tweets = soup.find_all('li','js-stream-item')\n",
    "      for tweet in tweets:\n",
    "        if tweet.find('p','tweet-text'):\n",
    "          tweet_user = tweet.find('span','username').text\n",
    "          tweet_text = tweet.find('p','tweet-text').text.encode('utf8')\n",
    "          tweet_id = tweet['data-item-id']\n",
    "          timestamp = tweet.find('a','tweet-timestamp')['title']\n",
    "          tweet_timestamp = datetime.datetime.strptime(timestamp, '%I:%M %p - %d %b %Y')\n",
    "\n",
    "          tweet_ids.append(tweet_id)\n",
    "          tweet_texts.append(tweet_text)\n",
    "          tweet_users.append(tweet_user)\n",
    "          tweet_timestamps.append(tweet_timestamp)\n",
    "          tweet_actions.append(action_list)\n",
    "          tweet_issues.append(issue_list)\n",
    "        if len(tweet_ids) % 25:\n",
    "          print 'In civil rights orgs'\n",
    "          print tweet_text\n",
    "          \n",
    "#### LGTBQ  Orgs\n",
    "for org in lgbtq_orgs['org_name']:\n",
    "  for issue_keyword in issue_types['LGTBQ rights']:\n",
    "    for category in action_categories.keys():\n",
    "    \n",
    "      action_list = action_categories[category]\n",
    "      issue_list = issue_types['LGTBQ rights']\n",
    "\n",
    "      tso = TwitterSearchOrder() # create a TwitterSearchOrder object\n",
    "      tso.set_keywords(action_list, or_operator = True)\n",
    "      tso.add_keyword('AND ' + issue_keyword)\n",
    "      tso.add_keyword('AND ' + org)\n",
    "      tso.set_language('en') \n",
    "      tso.set_include_entities(True) # and don't give us all those entity information\n",
    "      tso.set_result_type('recent')\n",
    "      tso.set_count(20)\n",
    "      querystr = tso.create_search_url() # create the url form the TwitterSearch package\n",
    "      queryURL = 'https://twitter.com/search?l=&'+ querystr[1:] + '+since:'+ weeks_ago.strftime(\"%Y-%m-%d\")\n",
    "      # Use selenium to get the URL\n",
    "      browser.get(queryURL)\n",
    "      for i in range (2):\n",
    "        # scroll to the bottom\n",
    "        browser.execute_script(\"window.scrollTo(0, 1000000)\")\n",
    "      # grab the html\n",
    "      html = browser.page_source\n",
    "      # use bs4 on the html\n",
    "      soup = soupy(html)\n",
    "\n",
    "      tweets = soup.find_all('li','js-stream-item')\n",
    "      for tweet in tweets:\n",
    "        if tweet.find('p','tweet-text'):\n",
    "          tweet_user = tweet.find('span','username').text\n",
    "          tweet_text = tweet.find('p','tweet-text').text.encode('utf8')\n",
    "          tweet_id = tweet['data-item-id']\n",
    "          timestamp = tweet.find('a','tweet-timestamp')['title']\n",
    "          tweet_timestamp = datetime.datetime.strptime(timestamp, '%I:%M %p - %d %b %Y')\n",
    "\n",
    "          tweet_ids.append(tweet_id)\n",
    "          tweet_texts.append(tweet_text)\n",
    "          tweet_users.append(tweet_user)\n",
    "          tweet_timestamps.append(tweet_timestamp)\n",
    "          tweet_actions.append(action_list)\n",
    "          tweet_issues.append(issue_list)\n",
    "        if len(tweet_ids) % 25:\n",
    "          print 'In LGBTQ orgs'\n",
    "          print tweet_text\n",
    "          \n",
    "\n",
    "tweets_df = pd.DataFrame(\n",
    "    {'tweet_ids': tweet_ids,\n",
    "     'tweet_texts': tweet_texts,\n",
    "     'tweet_users': tweet_users,\n",
    "     'tweet_timestamps': tweet_timestamps,\n",
    "     'tweet_actions': tweet_actions,\n",
    "     'tweet_issues': tweet_issues\n",
    "    })\n",
    "\n",
    "unique_tweets = tweets_df.drop_duplicates(subset=['tweet_ids'])\n",
    "\n",
    "print unique_tweets[:10]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       tweet_actions           tweet_ids  \\\n",
      "0  [event, meetup, huddle, congregate, gather, ga...  215277827484958720   \n",
      "1  [call, email, reach out, senator, representati...  780838022862471168   \n",
      "2  [event, meetup, huddle, congregate, gather, ga...   14999715963740161   \n",
      "\n",
      "                                        tweet_issues  \\\n",
      "0  [immigrant rights, immigrants' rights, refugee...   \n",
      "1  [immigrant rights, immigrants' rights, refugee...   \n",
      "2  [immigrant rights, immigrants' rights, refugee...   \n",
      "\n",
      "                                         tweet_texts    tweet_timestamps  \\\n",
      "0  Tomorrow (20 June) #IOM and #UNHCR organize a ... 2012-06-19 20:00:00   \n",
      "1  With H. E. Minister of Natural Disasters and R... 2016-09-27 11:34:00   \n",
      "2  Representatives of EU agencies and #UNHCR gath... 2010-12-15 03:06:00   \n",
      "\n",
      "       tweet_users  \n",
      "0  @EvelienBorgman  \n",
      "1          @nnnegm  \n",
      "2      @RefugeesCE  \n",
      "3\n"
     ]
    }
   ],
   "source": [
    "tweets_df = pd.DataFrame(\n",
    "    {'tweet_ids': tweet_ids,\n",
    "     'tweet_texts': tweet_texts,\n",
    "     'tweet_users': tweet_users,\n",
    "     'tweet_timestamps': tweet_timestamps,\n",
    "     'tweet_actions': tweet_actions,\n",
    "     'tweet_issues': tweet_issues\n",
    "    })\n",
    "\n",
    "unique_tweets = tweets_df.drop_duplicates(subset=['tweet_ids'])\n",
    "\n",
    "print unique_tweets[:10]\n",
    "print len(unique_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth_handler=auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "\n",
    "errors = 0\n",
    "\n",
    "with open(os.path.join(os.pardir, 'data/documents', 'tweepy_real_data_' + today.strftime(\"%Y%m%d%H%M\") + '.txt'), 'w') as outfile:\n",
    "  outfile.write('[')\n",
    "  for i,tweet_id in enumerate(list(unique_tweets.tweet_ids)):\n",
    "    tweet_status = api.get_status(int(tweet_id))\n",
    "    tweet = tweet_status._json\n",
    "    ntweets = 0\n",
    "    try:\n",
    "      ### Extract URL from Tweet if it exists\n",
    "      try:\n",
    "        if tweet.get('entities',False).get('media',False).get('urls',False):\n",
    "          tweet_urls = tweet['entities']['media']['urls']\n",
    "        else:\n",
    "          tweet_urls = ''\n",
    "      except:\n",
    "        tweet_urls = ''\n",
    "\n",
    "    ### Extract Date from Tweet if it exists\n",
    "      try:\n",
    "        tweet_date = dparser.parse(tweet['text'],fuzzy=True).strftime(\"%m/%d/%Y\")\n",
    "      except:\n",
    "        tweet_date = None\n",
    "    ### Extract Address from Tweet if it exists\n",
    "      try:\n",
    "        address_components = {}\n",
    "        for each in parse_address(tweet['text']):\n",
    "          address_components[each[1]] = each[0]\n",
    "        address = address_components.get('house_number','') + ' ' + address_components.get('road','')\n",
    "        city = address_components.get('city',None)\n",
    "        state = address_components.get('state',None)\n",
    "        zipcode = address_components.get('postcode',None)\n",
    "      except:\n",
    "        address = ''\n",
    "        city = None\n",
    "        state = None\n",
    "        zipcode = None\n",
    "    ### Extract City and State from User Location\n",
    "      try:\n",
    "        if city == None:\n",
    "          matches = re.findall(\"([\\w\\s]+),\\s(\\w+)\", string)\n",
    "          match = matches[0]\n",
    "          tweet_city = match[0]\n",
    "        if state == None:\n",
    "          matches = re.findall(\"([\\w\\s]+),\\s(\\w+)\", string)\n",
    "          match = matches[0]\n",
    "          tweet_state = match[1]\n",
    "      except:\n",
    "        tweet_city = ''\n",
    "        tweet_state = ''\n",
    "      ### Get Hashtags\n",
    "      try:\n",
    "        if tweet.get('entities',False).get('hashtags',False):\n",
    "          tweet_hashtags = tweet['entities']['hashtags']\n",
    "        else:\n",
    "          tweet_hashtags = []\n",
    "      except:\n",
    "        tweet_hashtags = []\n",
    "      ### Get User Mentions\n",
    "      try:\n",
    "        if tweet.get('entities',False).get('user_mentions',False):\n",
    "          tweet_users = tweet['entities']['user_mentions']\n",
    "        else:\n",
    "          tweet_users = []\n",
    "      except:\n",
    "        tweet_users = []\n",
    "\n",
    "      json_doc = {\n",
    "                        'posted_date': datetime.datetime.today().strftime(\"%m/%d/%Y\"),\n",
    "                        'expiration_date': tweet_date,\n",
    "                        'action_keywords': unique_tweets.iloc[[i],:].tweet_actions.values.tolist()[0], \n",
    "                        'issues_keywords': unique_tweets.iloc[[i],:].tweet_issues.values.tolist()[0], \n",
    "                        'document': {'content': tweet['text'],\n",
    "                                   'title': None,\n",
    "                                   'summary': None,\n",
    "                                   'url': 'https://twitter.com/statuses/' + str(tweet['id']),\n",
    "                                   'date': tweet['created_at']\n",
    "                                },\n",
    "                        'location': {'addr1': address,\n",
    "                                   'addr2': None,\n",
    "                                   'city':  tweet_city,\n",
    "                                   'state': tweet_state,\n",
    "                                   'zip':  zipcode ,\n",
    "                                   'lat':   None,\n",
    "                                   'long':  None,\n",
    "                                   },\n",
    "                        'org_name': org,\n",
    "\n",
    "                        # your mileage may vary\n",
    "                        'other_metadata': {\n",
    "                            'twitter_coordinates': tweet['coordinates'],\n",
    "                            'twitter_urls': tweet_urls,\n",
    "                            'twitter_user_mentions': tweet_users,\n",
    "                            'twitter_hashtags': tweet_hashtags,\n",
    "                            'twitter_favorite_count': tweet['favorite_count'],\n",
    "                            'twitter_retweet_count': tweet['retweet_count']\n",
    "                            }\n",
    "                    }\n",
    "\n",
    "      new_record = {\n",
    "                            \"_index\" : \"twitter\",\n",
    "                            \"_type\"  : \"tweet\",\n",
    "                            \"_id\"    : tweet['id'],\n",
    "                            \"_source\": json_doc,\n",
    "                        }\n",
    "\n",
    "      json.dump(new_record, outfile)\n",
    "      outfile.write(',')\n",
    "      ntweets += 1\n",
    "      if ntweets % 100:\n",
    "        print( tweet['text']  )\n",
    "\n",
    "    except error as e: # take care of all those ugly errors if there are some\n",
    "      print(e)\n",
    "      errors += 1\n",
    "      if errors == 5:\n",
    "        break\n",
    "      time.sleep(60*3)\n",
    "\n",
    "  outfile.seek(-1, os.SEEK_END)\n",
    "  outfile.truncate()\n",
    "  outfile.write(']')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
