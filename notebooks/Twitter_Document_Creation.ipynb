{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gnp\n",
    "import codecs\n",
    "import json\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.key import Key\n",
    "import boto\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "from TwitterSearch import *\n",
    "from sunlight import congress\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import itertools\n",
    "import datetime\n",
    "import configparser\n",
    "import time\n",
    "from pattern.en import parsetree\n",
    "from pattern.en import mood\n",
    "import dateutil.parser as dparser\n",
    "import sunlight\n",
    "from sunlight import congress\n",
    "import re\n",
    "from postal.parser import parse_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:2: DeprecationWarning: You passed a bytestring as `filenames`. This will not work on Python 3. Use `cp.read_file()` or switch to using Unicode strings across the board.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.join(os.pardir, 'config', 'ross.ini'))\n",
    "\n",
    "consumer_key = config['twitter.api']['consumer_key']\n",
    "consumer_secret = config['twitter.api']['consumer_secret']\n",
    "access_token = config['twitter.api']['access_token']\n",
    "access_token_secret = config['twitter.api']['access_token_secret']\n",
    "\n",
    "AWS_ACCESS_KEY_ID = config['aws.creds']['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = config['aws.creds']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP [(u'You', u'PRP')]\n",
      "VP [(u'should', u'MD'), (u'call', u'VB')]\n",
      "NP [(u'your', u'PRP$'), (u'senator', u'NN'), (u'today', u'NN')]\n"
     ]
    }
   ],
   "source": [
    "s = parsetree('You should call your senator today.')\n",
    "for sentence in s:\n",
    "  for chunk in sentence.chunks:\n",
    "    print chunk.type, [(w.string, w.type) for w in chunk.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imperative'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mood('Call your senator today!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# issues = [\"civil rights\",\n",
    "# \"women's rights\",\"planned parenthood\",\n",
    "# \"black lives matter\",\"african american rights\",\"black rights\",\n",
    "# \"immigration\",\"immigrants\",\"undocumented\",\"border wall\",\"refugees\",\"refugee rights\",\n",
    "# \"gun control\",\n",
    "# \"public education\",\n",
    "# \"LGBTQ rights\",\"LGBTQ\",\"LGBT\",\n",
    "# \"voting rights\",\"redistricting\",\"voter id laws\",\"voting access\",\"voter access\"\n",
    "#          ]\n",
    "\n",
    "\n",
    "action_categories = {'Donation': [\"donation\",\"give\",\"donate\",\"give support\",\"financial support\",\"contribute\",\"contributing\"],\n",
    "                     'Protest': [\"protest\",\"rally\",\"demonstration\",\"demonstrate\"],\n",
    "                     'Gathering': [\"meetup\",\"huddle\",\"congregate\",\"gather\",\"gathering\",\"discuss\",\"discussion\"],\n",
    "                     'Boycott': [\"boycott\"],\n",
    "                     'Advocate': [\"call\",\"email\",\"reach out\",\"senator\",\"representative\", \"sign petition\",\"petition\"],\n",
    "                     'Vote': [\"vote\",\"cast your ballot\"],\n",
    "                     'Townhall': [\"town hall\",\"open office\",\"town meeting\",\"townhall\",'virtual townhall','listening session']\n",
    "                          }\n",
    "\n",
    "issue_types = { 'immigrant rights' : ['immigrant rights',\"immigrants' rigths\",'refugee rights', 'travel ban',\n",
    "                                        'border wall','refugees','asylum','immigration reform',\n",
    "                                        'immigrant advocacy','migrant rights'],\n",
    "               \"women's rights\" : [\"women's rights\",\"women's rights\",'womens rights','gender equality'\n",
    "                                  'girl power',\"international women's day\",\"war on women\",\"planned parenthood\"],\n",
    "               \"black rights\" : [\"racial equality\",\"black lives matter\",\"african american rights\",\"civil rights\",\n",
    "                                \"black power\",\"jim crow\"],\n",
    "               \"LGTBQ rights\" : [\"marriage equality\",\"transgender rights\",\"equality act\",\"lesbian rights\",'gay rights',\n",
    "                                'bisexual rights'],\n",
    "               \"voting rights\": [\"redistricting\",\"gerrymandering\",\"redistrict\",\"gerrymander\",\"voter id\",\"voting access\",\n",
    "                                \"voter access\",\"voter suppression\"]\n",
    "                      }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['town hall', 'open office', 'town meeting', 'townhall', 'virtual townhall', 'listening session', 'Ross']\n"
     ]
    }
   ],
   "source": [
    "action_categories.keys()\n",
    "listed = action_categories['Townhall']\n",
    "listed.append('Ross')\n",
    "print listed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'title', u'full_title', u'firstname', u'middlename', u'lastname',\n",
      "       u'name_suffix', u'nickname', u'party', u'state', u'district',\n",
      "       u'in_office', u'gender', u'phone', u'fax', u'website', u'webform',\n",
      "       u'congress_office', u'bioguide_id', u'votesmart_id', u'fec_id',\n",
      "       u'govtrack_id', u'crp_id', u'twitter_id', u'congresspedia_url',\n",
      "       u'youtube_url', u'facebook_id', u'official_rss', u'senate_class',\n",
      "       u'birthdate', u'oc_email', u'title_name'],\n",
      "      dtype='object')\n",
      "\n",
      "['Representative Robert Aderholt', 'Senator Lamar Alexander', 'Representative Justin Amash', 'Representative Mark Amodei', 'Representative Alma Adams', 'Representative Pete Aguilar', 'Representative Rick Allen', 'Representative Ralph Abraham', 'Representative Jodey Arrington', 'Representative Joe Barton']\n"
     ]
    }
   ],
   "source": [
    "legislators = pd.read_csv(os.path.join(os.pardir, 'data/static_data', 'legislators.csv'))\n",
    "\n",
    "mask = (legislators.in_office == 1)\n",
    "\n",
    "legislators = legislators.loc[mask]\n",
    "\n",
    "legislators['title_name'] = legislators['full_title'] + ' ' + legislators['firstname'] + ' ' + legislators['lastname']\n",
    "\n",
    "print legislators.columns\n",
    "print\n",
    "print list(legislators['title_name'])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "immigration_orgs = pd.read_csv(os.path.join(os.pardir, 'data/static_data', 'immigration_orgs.csv'))\n",
    "\n",
    "# for org in immigration_orgs['org_name']:\n",
    "#   print org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'issues' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-00e464351220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mkeyword_searches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcombination\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_categories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0missues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mkeyword_searches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'issues' is not defined"
     ]
    }
   ],
   "source": [
    "leg_names = list(legislators['title_name'])\n",
    "leg_states = list(legislators['state'])\n",
    "leg_twitter = list(legislators['twitter_id'])\n",
    "\n",
    "keyword_searches = []\n",
    "\n",
    "for combination in itertools.product(action_categories, issues):\n",
    "  keyword_searches.append(list(combination))\n",
    "  \n",
    "print keyword_searches[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 429: ('Too Many Requests: Request cannot be served ', \"due to the application's rate limit having \", 'been exhausted for the resource')\n",
      "Error 429: ('Too Many Requests: Request cannot be served ', \"due to the application's rate limit having \", 'been exhausted for the resource')\n",
      "RT @IOMAustralia: TOGETHER! IOM, UNIC and UNHCR at the Canberra Multicultural\n",
      "Festival. Many join each year to celebrate culture!#UN4Refuge…\n",
      "RT @IOMAustralia: TOGETHER! IOM, UNIC and UNHCR at the Canberra Multicultural\n",
      "Festival. Many join each year to celebrate culture!#UN4Refuge…\n",
      "RT @UNHCR_SriLanka: What do you want to ask internationally acclaimed writer and UNHCR supporter Neil Gaiman? Join us, and host... https://…\n",
      "RT @Aussie4Refugees: Refugee Council tweet this horror, and UNHCR claim \"profound concern\", so why not call for an END TO THE POLICY?  http…\n",
      "Error 429: ('Too Many Requests: Request cannot be served ', \"due to the application's rate limit having \", 'been exhausted for the resource')\n",
      "Error 429: ('Too Many Requests: Request cannot be served ', \"due to the application's rate limit having \", 'been exhausted for the resource')\n",
      "Error 429: ('Too Many Requests: Request cannot be served ', \"due to the application's rate limit having \", 'been exhausted for the resource')\n",
      "Error 429: ('Too Many Requests: Request cannot be served ', \"due to the application's rate limit having \", 'been exhausted for the resource')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6ea041eb2963>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0merrors\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m   \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSEEK_END\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m   \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "today = datetime.datetime.today()\n",
    "weeks_ago = today - datetime.timedelta(days=14)\n",
    "errors = 0\n",
    "sleep_for = 3*60 # sleep for 3 min\n",
    "last_amount_of_queries = 0 # used to detect when new queries are done\n",
    "\n",
    "  \n",
    "with open(os.path.join(os.pardir, 'data/documents', 'real_data_' + today.strftime(\"%Y%m%d%H%M\") + '.txt'), 'w') as outfile:\n",
    "  outfile.write('[')\n",
    "  ntweets = 0\n",
    "\n",
    "  for org in immigration_orgs['org_name']:\n",
    "    for category in action_categories.keys():\n",
    "      listed = action_categories[category]\n",
    "      try:\n",
    "        tso = TwitterSearchOrder() # create a TwitterSearchOrder object\n",
    "  #         tso.set_keywords(action_categories[category], or_operator = True) # let's define all words we would like to have a look for\n",
    "        tso.set_keywords(listed, or_operator = True)\n",
    "        tso.add_keyword('AND '+org)\n",
    "        tso.add_keyword('AND -filter:retweets')\n",
    "        tso.set_language('en') # we want to see German tweets only\n",
    "        tso.set_include_entities(True) # and don't give us all those entity information\n",
    "        tso.set_result_type('recent')\n",
    "  #       set_geocode(52.5233,13.4127,10,imperial_metric=True)\n",
    "        tso.set_count(10)\n",
    "  #       tso.set_link_filter()\n",
    "        querystr = tso.create_search_url()\n",
    "\n",
    "\n",
    "        tso2 = TwitterSearchOrder()\n",
    "        tso2.set_search_url(querystr + '+since:'+ weeks_ago.strftime(\"%Y-%m-%d\"))\n",
    "  #        -filter:retweets\n",
    "          # it's about time to create a TwitterSearch object with our secret tokens\n",
    "        ts = TwitterSearch(\n",
    "            consumer_key,\n",
    "            consumer_secret,\n",
    "            access_token,\n",
    "            access_token_secret\n",
    "         )\n",
    "\n",
    "        ### this is where the fun actually starts :)\n",
    "        for i,tweet in enumerate(ts.search_tweets_iterable(tso2)):\n",
    "          \n",
    "#           ### Extract URL from Tweet if it exists\n",
    "#           try:\n",
    "#             if tweet.get('entities',False).get('media',False):\n",
    "#               tweet_url = tweet['entities']['media'][0]['url']\n",
    "#             else:\n",
    "#               tweet_url = ''\n",
    "#           except:\n",
    "#             tweet_url = ''\n",
    "\n",
    "        ### Extract Date from Tweet if it exists\n",
    "          try:\n",
    "            tweet_date = dparser.parse(tweet['text'],fuzzy=True).strftime(\"%m/%d/%Y\")\n",
    "          except:\n",
    "            tweet_date = None\n",
    "        ### Extract Address from Tweet if it exists\n",
    "          try:\n",
    "            address_components = {}\n",
    "            for each in parse_address(tweet['text']):\n",
    "              address_components[each[1]] = each[0]\n",
    "            address = address_components.get('house_number','') + ' ' + address_components.get('road','')\n",
    "            city = address_components.get('city',None)\n",
    "            state = address_components.get('state',None)\n",
    "            zipcode = address_components.get('postcode',None)\n",
    "          except:\n",
    "            address = ''\n",
    "            city = None\n",
    "            state = None\n",
    "            zipcode = None\n",
    "        ### Extract City and State from User Location\n",
    "          try:\n",
    "            if city == None:\n",
    "              matches = re.findall(\"([\\w\\s]+),\\s(\\w+)\", string)\n",
    "              match = matches[0]\n",
    "              tweet_city = match[0]\n",
    "            if state == None:\n",
    "              matches = re.findall(\"([\\w\\s]+),\\s(\\w+)\", string)\n",
    "              match = matches[0]\n",
    "              tweet_state = match[1]\n",
    "          except:\n",
    "            tweet_city = ''\n",
    "            tweet_state = ''\n",
    "          ### Get Hashtags\n",
    "          try:\n",
    "            if tweet.get('entities',False).get('hashtags',False):\n",
    "              tweet_hashtags = tweet['entities']['hashtags']\n",
    "            else:\n",
    "              tweet_hashtags = []\n",
    "          except:\n",
    "            tweet_hashtags = []\n",
    "\n",
    "          json_doc = {\n",
    "                            'posted_date': datetime.datetime.today().strftime(\"%m/%d/%Y\"),\n",
    "                            'expiration_date': tweet_date,\n",
    "                            'action_keywords': action_categories[category], \n",
    "                            'issues_keywords': issue_types['immigrant rights'], \n",
    "                            'document': {'content': tweet['text'],\n",
    "                                       'title': None,\n",
    "                                       'summary': None,\n",
    "                                       'url': 'https://twitter.com/statuses/' + str(tweet['id']),\n",
    "                                       'date': tweet['created_at']\n",
    "                                    },\n",
    "                            'location': {'addr1': address,\n",
    "                                       'addr2': None,\n",
    "                                       'city':  tweet_city,\n",
    "                                       'state': tweet_state,\n",
    "                                       'zip':  zipcode ,\n",
    "                                       'lat':   None,\n",
    "                                       'long':  None,\n",
    "                                       },\n",
    "                            'org_name': org,\n",
    "\n",
    "                            # your mileage may vary\n",
    "                            'other_metadata': {\n",
    "                                'twitter_coordinates': tweet['coordinates'],\n",
    "                                'twitter_hashtags': tweet_hashtags,\n",
    "                                'twitter_favorite_count': tweet['favorite_count'],\n",
    "                                'twitter_retweet_count': tweet['retweet_count']\n",
    "                                }\n",
    "                        }\n",
    "\n",
    "          new_record = {\n",
    "                                \"_index\" : \"twitter\",\n",
    "                                \"_type\"  : \"tweet\",\n",
    "                                \"_id\"    : tweet['id'],\n",
    "                                \"_source\": json_doc,\n",
    "                            }\n",
    "\n",
    "          json.dump(new_record, outfile)\n",
    "          outfile.write(',')\n",
    "          ntweets += 1\n",
    "          if ntweets % 100:\n",
    "            print( tweet['text']  )\n",
    "          \n",
    "          ### Manage Rate Limits\n",
    "          current_amount_of_queries = ts.get_statistics()[0]\n",
    "          if not last_amount_of_queries == current_amount_of_queries:\n",
    "            last_amount_of_queries = current_amount_of_queries\n",
    "            time.sleep(sleep_for)\n",
    "\n",
    "      except TwitterSearchException as e: # take care of all those ugly errors if there are some\n",
    "        if errors == 5:\n",
    "          break\n",
    "        print(e)\n",
    "        errors + 1\n",
    "        time.sleep(60*15)\n",
    "  outfile.seek(-1, os.SEEK_END)\n",
    "  outfile.truncate()\n",
    "  outfile.write(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#,'ACLU', 'CAPAction'\n",
    "\n",
    "# try:\n",
    "#     tuo = TwitterUserOrder('ACLU') # create a TwitterUserOrder\n",
    "    \n",
    "#     # it's about time to create TwitterSearch object again\n",
    "#     ts = TwitterSearch(\n",
    "#           consumer_key,\n",
    "#           consumer_secret,\n",
    "#           access_token,\n",
    "#           access_token_secret\n",
    "#     )\n",
    "\n",
    "#     # start asking Twitter about the timeline\n",
    "#     for i,tweet in enumerate(ts.search_tweets_iterable(tuo)):\n",
    "#       print( '@%s tweeted: %s' % ( tweet['user']['screen_name'], tweet['text'] ) )       \n",
    "#       if i > 20:\n",
    "#         break\n",
    "\n",
    "# except TwitterSearchException as e: # catch all those ugly errors\n",
    "#     print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data.txt to Amazon S3 bucket mids-capstone-rzst\n"
     ]
    }
   ],
   "source": [
    "# conn = S3Connection(AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY)\n",
    "# bucket = conn.get_bucket('mids-w205-assignment-3-new-tweets')\n",
    "# bucket_list = bucket.list()\n",
    "\n",
    "bucket_name = 'mids-capstone-rzst'\n",
    "conn = boto.connect_s3(AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "\n",
    "# bucket = conn.create_bucket(bucket_name,\n",
    "#     location=boto.s3.connection.Location.DEFAULT)\n",
    "\n",
    "testfile = \"data.txt\"\n",
    "print 'Uploading %s to Amazon S3 bucket %s' % \\\n",
    "   (testfile, bucket_name)\n",
    "\n",
    "def percent_cb(complete, total):\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "# k = Key(bucket_name)\n",
    "# k.key = 'data.txt'\n",
    "# k.set_contents_from_filename(testfile,\n",
    "#     cb=percent_cb, num_cb=10)\n",
    "\n",
    "\n",
    "# s3_connection = boto.connect_s3()\n",
    "bucket = conn.get_bucket('mids-capstone-rzst')\n",
    "key = boto.s3.key.Key(bucket, 'data.txt')\n",
    "key.set_contents_from_filename('data.txt')\n",
    "key.set_acl('public-read')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
